import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse

def is_valid_url(url):
    """Check if the URL format is correct."""
    parsed = urlparse(url)
    return all([parsed.scheme, parsed.netloc])

def scrape_and_clean(url, chunk_size=500, only_gov=False):
    try:
        # Validate URL format
        if not is_valid_url(url):
            raise ValueError(f"Invalid URL format: {url}")

        # Check for .gov sites if required
        if only_gov and not urlparse(url).netloc.endswith(".gov.in"):
            raise ValueError(f"URL is not a .gov site: {url}")

        # Fetch page content
        response = requests.get(url, timeout=10)
        response.raise_for_status()  # Raises HTTPError if status 4xx/5xx

        # Extract and clean text
        soup = BeautifulSoup(response.text, 'html.parser')
        text = soup.get_text(separator=" ", strip=True)

        # Split text into chunks
        chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
        return chunks

    except requests.exceptions.MissingSchema:
        print(f"❌ Invalid URL format: {url}")
    except requests.exceptions.ConnectionError:
        print(f"❌ Could not connect to: {url}")
    except requests.exceptions.Timeout:
        print(f"⏳ Timeout while fetching: {url}")
    except requests.exceptions.HTTPError as e:
        print(f"❌ HTTP error: {e}")
    except ValueError as e:
        print(f"❌ {e}")
    except Exception as e:
        print(f"⚠️ Unexpected error: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python main.py <URL> [--only-gov]")
        sys.exit(1)

    url = sys.argv[1]
    only_gov_flag = "--only-gov" in sys.argv

    chunks = scrape_and_clean(url, chunk_size=500, only_gov=only_gov_flag)

    if chunks:
        print(f"✅ Scraped {len(chunks)} chunks from {url}")
        for i, chunk in enumerate(chunks, 1):
            print(f"\n--- Chunk {i} ---\n{chunk}")
